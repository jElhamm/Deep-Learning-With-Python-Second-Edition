{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmRAEtvJ_6OT"
      },
      "source": [
        "# **Deep Learning With Python  -  CHAPTER 13**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HE2uMKK_7Qp"
      },
      "source": [
        "- This Python script implements a hyperparameter tuning and training pipeline for a simple Multi-Layer Perceptron (MLP) model using TensorFlow and Keras Tuner.\n",
        "\n",
        "- The code is structured into modular classes: `SimpleMLP` defines the neural network architecture, `HyperparameterTuner` manages the hyperparameter search using Bayesian optimization, and `ModelTrainer` handles model training and evaluation.\n",
        "\n",
        "- The MNIST dataset is preprocessed and split into training, validation, and test sets. The best hyperparameter configurations are identified, and models are trained using an optimal number of epochs determined through early stopping.\n",
        "\n",
        "- Finally, the best models are evaluated on the test set to ensure optimal performance. This structured approach enhances code readability, reusability, and maintainability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oqNg9frRCBgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98b0a7d-e4e8-4f70-c1e6-a9ceb8f5360b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/129.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7HHCQEtlu8Pg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMLP(kt.HyperModel):\n",
        "    def __init__(self, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build(self, hp):\n",
        "        units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n",
        "        optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
        "\n",
        "        model = keras.Sequential([\n",
        "            layers.Dense(units, activation=\"relu\"),\n",
        "            layers.Dense(self.num_classes, activation=\"softmax\")\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=\"sparse_categorical_crossentropy\",\n",
        "            metrics=[\"accuracy\"]\n",
        "        )\n",
        "        return model"
      ],
      "metadata": {
        "id": "6mitfoIAJY0r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperparameterTuner:\n",
        "    def __init__(self, hypermodel, max_trials=100, executions_per_trial=2, directory=\"mnist_kt_test\"):\n",
        "        self.tuner = kt.BayesianOptimization(\n",
        "            hypermodel,\n",
        "            objective=\"val_accuracy\",\n",
        "            max_trials=max_trials,\n",
        "            executions_per_trial=executions_per_trial,\n",
        "            directory=directory,\n",
        "            overwrite=True\n",
        "        )\n",
        "\n",
        "    def search(self, x_train, y_train, x_val, y_val):\n",
        "        callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)]\n",
        "\n",
        "        self.tuner.search(\n",
        "            x_train, y_train,\n",
        "            batch_size=128,\n",
        "            epochs=100,\n",
        "            validation_data=(x_val, y_val),\n",
        "            callbacks=callbacks,\n",
        "            verbose=2\n",
        "        )\n",
        "\n",
        "    def get_best_hyperparameters(self, top_n=4):\n",
        "        return self.tuner.get_best_hyperparameters(top_n)\n",
        "\n",
        "    def get_best_models(self, top_n=4):\n",
        "        return self.tuner.get_best_models(top_n)"
      ],
      "metadata": {
        "id": "qr8jz4GeLVbT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTrainer:\n",
        "    def __init__(self, x_train_full, y_train_full, x_train, y_train, x_val, y_val):\n",
        "        self.x_train_full = x_train_full\n",
        "        self.y_train_full = y_train_full\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_val = x_val\n",
        "        self.y_val = y_val\n",
        "\n",
        "    def get_best_epoch(self, model_builder, hp):\n",
        "        model = model_builder.build(hp)\n",
        "        callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)]\n",
        "\n",
        "        history = model.fit(\n",
        "            self.x_train, self.y_train,\n",
        "            validation_data=(self.x_val, self.y_val),\n",
        "            epochs=100,\n",
        "            batch_size=128,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        val_loss_per_epoch = history.history[\"val_loss\"]\n",
        "        best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
        "        print(f\"Best epoch: {best_epoch}\")\n",
        "        return best_epoch\n",
        "\n",
        "    def get_best_trained_model(self, model_builder, hp):\n",
        "        \"\"\"آموزش مدل با بهترین تعداد epoch.\"\"\"\n",
        "        best_epoch = self.get_best_epoch(model_builder, hp)\n",
        "        model = model_builder.build(hp)\n",
        "\n",
        "        model.fit(\n",
        "            self.x_train_full, self.y_train_full,\n",
        "            batch_size=128,\n",
        "            epochs=int(best_epoch * 1.2)\n",
        "        )\n",
        "        return model"
      ],
      "metadata": {
        "id": "5KU0n2MpLXsb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data():\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "    x_train = x_train.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
        "    x_test = x_test.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
        "\n",
        "    x_train_full = x_train[:]\n",
        "    y_train_full = y_train[:]\n",
        "\n",
        "    num_val_samples = 10000\n",
        "    x_train, x_val = x_train[:-num_val_samples], x_train[-num_val_samples:]\n",
        "    y_train, y_val = y_train[:-num_val_samples], y_train[-num_val_samples:]\n",
        "\n",
        "    return x_train_full, y_train_full, x_train, y_train, x_val, y_val, x_test, y_test"
      ],
      "metadata": {
        "id": "DkfZR6EULaJ6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "x_train_full, y_train_full, x_train, y_train, x_val, y_val, x_test, y_test = prepare_data()\n",
        "\n",
        "hypermodel = SimpleMLP(num_classes=10)\n",
        "tuner = HyperparameterTuner(hypermodel)\n",
        "tuner.search(x_train, y_train, x_val, y_val)\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(top_n=4)\n",
        "trainer = ModelTrainer(x_train_full, y_train_full, x_train, y_train, x_val, y_val)\n",
        "\n",
        "best_models = []\n",
        "for hp in best_hps:\n",
        "    model = trainer.get_best_trained_model(hypermodel, hp)\n",
        "    model.evaluate(x_test, y_test)\n",
        "    best_models.append(model)\n",
        "\n",
        "best_models = tuner.get_best_models(top_n=4)"
      ],
      "metadata": {
        "id": "0azlwDjrSIvw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}