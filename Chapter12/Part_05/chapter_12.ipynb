{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmRAEtvJ_6OT"
      },
      "source": [
        "# **Deep Learning With Python  -  CHAPTER 12**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HE2uMKK_7Qp"
      },
      "source": [
        "- This code implements a **Generative Adversarial Network (GAN)** to generate realistic face images using the **CelebA dataset**.\n",
        "\n",
        "- It is structured into several classes for better modularity:\n",
        "  \n",
        "  `CelebAGANDataLoader` handles dataset downloading and preprocessing, `Discriminator` and `Generator` define the two neural networks, `GAN` encapsulates the training logic, and `GANMonitor` saves generated images during training.\n",
        "\n",
        "- The model is trained using **binary cross-entropy loss**, and the generator learns to create increasingly realistic images by competing with the discriminator. The training loop runs for **100 epochs**, generating images at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oqNg9frRCBgk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7HHCQEtlu8Pg"
      },
      "outputs": [],
      "source": [
        "class CelebAGANDataLoader:\n",
        "    def __init__(self, dataset_path=\"celeba_gan\", image_size=(64, 64), batch_size=32):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def download_and_extract_data(self):\n",
        "        os.makedirs(self.dataset_path, exist_ok=True)\n",
        "        !gdown --id 1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684 -O {self.dataset_path}/data.zip\n",
        "        !unzip -qq {self.dataset_path}/data.zip -d {self.dataset_path}\n",
        "\n",
        "    def load_dataset(self):\n",
        "        dataset = keras.utils.image_dataset_from_directory(\n",
        "            self.dataset_path,\n",
        "            label_mode=None,\n",
        "            image_size=self.image_size,\n",
        "            batch_size=self.batch_size,\n",
        "            smart_resize=True\n",
        "        )\n",
        "        return dataset.map(lambda x: x / 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(keras.Model):\n",
        "    def __init__(self, input_shape=(64, 64, 3)):\n",
        "        super().__init__()\n",
        "        self.model = keras.Sequential([\n",
        "            layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Flatten(),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(1, activation=\"sigmoid\"),\n",
        "        ], name=\"discriminator\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)"
      ],
      "metadata": {
        "id": "6mitfoIAJY0r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(keras.Model):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.model = keras.Sequential([\n",
        "            layers.Dense(8 * 8 * 128, input_shape=(latent_dim,)),\n",
        "            layers.Reshape((8, 8, 128)),\n",
        "            layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "        ], name=\"generator\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)"
      ],
      "metadata": {
        "id": "qr8jz4GeLVbT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
        "\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\"d_loss\": self.d_loss_metric.result(), \"g_loss\": self.g_loss_metric.result()}"
      ],
      "metadata": {
        "id": "5KU0n2MpLXsb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images = generated_images.numpy()\n",
        "\n",
        "        for i in range(self.num_img):\n",
        "            img = keras.utils.array_to_img(generated_images[i])\n",
        "            img.save(f\"generated_img_{epoch:03d}_{i}.png\")"
      ],
      "metadata": {
        "id": "DkfZR6EULaJ6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    data_loader = CelebAGANDataLoader()\n",
        "    dataset = data_loader.load_dataset()\n",
        "\n",
        "    discriminator = Discriminator()\n",
        "    generator = Generator(latent_dim=128)\n",
        "\n",
        "    gan = GAN(discriminator=discriminator, generator=generator, latent_dim=128)\n",
        "    gan.compile(\n",
        "        d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss_fn=keras.losses.BinaryCrossentropy(),\n",
        "    )\n",
        "\n",
        "    gan.fit(dataset, epochs=100, callbacks=[GANMonitor(num_img=10, latent_dim=128)])"
      ],
      "metadata": {
        "id": "8J0DSS30O0uZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}