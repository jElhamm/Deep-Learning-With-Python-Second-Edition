{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmRAEtvJ_6OT"
      },
      "source": [
        "# **Deep Learning With Python  -  CHAPTER 12**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HE2uMKK_7Qp"
      },
      "source": [
        "- This code implements a transformer-based text generation model using TensorFlow and Keras. It starts by downloading and preprocessing the **IMDB dataset**, removing unnecessary HTML tags, and converting the text into tokenized sequences using **TextVectorization**.\n",
        "\n",
        "- The model consists of a **Transformer Decoder** with **positional embeddings** to capture word order dependencies. A **temperature-based sampling strategy** is employed for generating diverse text outputs.\n",
        "\n",
        "- The training loop utilizes **multi-head attention** and **causal masking** to ensure that the model generates text in an autoregressive manner. The text generation process is guided by a **custom callback**, which generates sequences at different temperature values to showcase controlled randomness in the output.\n",
        "\n",
        "- This structured and modular approach makes it a powerful architecture for **text generation and NLP applications**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oqNg9frRCBgk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7HHCQEtlu8Pg"
      },
      "outputs": [],
      "source": [
        "class DatasetPreparer:\n",
        "    \"\"\"Downloads, extracts, and preprocesses the IMDB dataset for language modeling.\"\"\"\n",
        "    @staticmethod\n",
        "    def download_and_extract():\n",
        "        os.system(\"wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
        "        os.system(\"tar -xf aclImdb_v1.tar.gz\")\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_text(text):\n",
        "        \"\"\"Removes HTML tags from the text.\"\"\"\n",
        "        return tf.strings.regex_replace(text, \"<br />\", \" \")\n",
        "\n",
        "    def prepare_dataset(self, directory=\"aclImdb\", batch_size=256):\n",
        "        \"\"\"Loads the dataset and cleans the text.\"\"\"\n",
        "        dataset = keras.utils.text_dataset_from_directory(directory=directory, label_mode=None, batch_size=batch_size)\n",
        "        dataset = dataset.map(lambda x: self.clean_text(x))\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lx1G3oUPCicE"
      },
      "outputs": [],
      "source": [
        "class TextVectorizer:\n",
        "    \"\"\"Handles tokenization and vectorization of text data.\"\"\"\n",
        "    def __init__(self, vocab_size=15000, sequence_length=100):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vectorizer = layers.TextVectorization(\n",
        "            max_tokens=self.vocab_size,\n",
        "            output_mode=\"int\",\n",
        "            output_sequence_length=self.sequence_length\n",
        "        )\n",
        "\n",
        "    def adapt(self, dataset):\n",
        "        \"\"\"Fits the vectorizer to the dataset.\"\"\"\n",
        "        self.vectorizer.adapt(dataset)\n",
        "\n",
        "    def transform(self, text_batch):\n",
        "        \"\"\"Transforms text into integer sequences.\"\"\"\n",
        "        vectorized_sequences = self.vectorizer(text_batch)\n",
        "        x = vectorized_sequences[:, :-1]\n",
        "        y = vectorized_sequences[:, 1:]\n",
        "        return x, y\n",
        "\n",
        "    def get_vectorized_dataset(self, dataset):\n",
        "        \"\"\"Returns a dataset with transformed sequences.\"\"\"\n",
        "        return dataset.map(lambda x: self.transform(x), num_parallel_calls=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    \"\"\"Applies token and positional embeddings to the input text.\"\"\"\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    \"\"\"Implements a Transformer decoder block with self-attention and feed-forward layers.\"\"\"\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        \"\"\"Generates a causal mask to prevent the model from looking ahead.\"\"\"\n",
        "        input_shape = tf.shape(inputs)\n",
        "        sequence_length = input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        return tf.tile(mask[tf.newaxis, :, :], [tf.shape(inputs)[0], 1, 1])\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        padding_mask = mask if mask is None else tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "        padding_mask = tf.minimum(padding_mask, causal_mask) if mask is not None else causal_mask\n",
        "\n",
        "        attention_output_1 = self.attention_1(query=inputs, value=inputs, key=inputs, attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "AMIFlb8B5DTO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerTextGenerator:\n",
        "    \"\"\"Builds and compiles the transformer-based text generation model.\"\"\"\n",
        "    @staticmethod\n",
        "    def build(sequence_length, vocab_size, embed_dim=256, dense_dim=2048, num_heads=2):\n",
        "        inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "        x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "        x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, x)\n",
        "        outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")\n",
        "        return model"
      ],
      "metadata": {
        "id": "zI1CcA9A5Fxm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"Generates text at different temperatures at the end of each epoch.\"\"\"\n",
        "    def __init__(self, prompt, generate_length, model_input_length, vectorizer, temperatures=(1.0,), print_freq=1):\n",
        "        self.prompt = prompt\n",
        "        self.generate_length = generate_length\n",
        "        self.model_input_length = model_input_length\n",
        "        self.temperatures = temperatures\n",
        "        self.print_freq = print_freq\n",
        "        self.vectorizer = vectorizer\n",
        "        self.tokens_index = dict(enumerate(self.vectorizer.vectorizer.get_vocabulary()))\n",
        "        vectorized_prompt = self.vectorizer.vectorizer([prompt])[0].numpy()\n",
        "        self.prompt_length = np.nonzero(vectorized_prompt == 0)[0][0]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.print_freq != 0:\n",
        "            return\n",
        "\n",
        "        for temperature in self.temperatures:\n",
        "            print(f\"\\n== Generating with temperature {temperature} ==\")\n",
        "            sentence = self.prompt\n",
        "\n",
        "            for _ in range(self.generate_length):\n",
        "                tokenized_sentence = self.vectorizer.vectorizer([sentence])\n",
        "                predictions = self.model(tokenized_sentence)\n",
        "                next_token = self._sample_next(predictions[0, self.prompt_length - 1, :], temperature)\n",
        "                sentence += \" \" + self.tokens_index[next_token]\n",
        "\n",
        "            print(sentence)\n",
        "\n",
        "    @staticmethod\n",
        "    def _sample_next(predictions, temperature=1.0):\n",
        "        predictions = np.log(predictions) / temperature\n",
        "        exp_preds = np.exp(predictions)\n",
        "        predictions = exp_preds / np.sum(exp_preds)\n",
        "        return np.argmax(np.random.multinomial(1, predictions, 1))"
      ],
      "metadata": {
        "id": "FjL17ZVC5Iw-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DatasetPreparer.download_and_extract()\n",
        "dataset_prep = DatasetPreparer()\n",
        "dataset = dataset_prep.prepare_dataset()"
      ],
      "metadata": {
        "id": "XSAgrObR5KwV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de31a2a-c613-4852-f3f6-950c9c870214"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 105006 files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TextVectorizer()\n",
        "vectorizer.adapt(dataset)\n",
        "lm_dataset = vectorizer.get_vectorized_dataset(dataset)"
      ],
      "metadata": {
        "id": "iUSpYl9HBvpj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerTextGenerator.build(sequence_length=100, vocab_size=15000)\n",
        "text_gen_callback = TextGenerator(prompt=\"This movie\", generate_length=50, model_input_length=100,\n",
        "                                  vectorizer=vectorizer, temperatures=(0.2, 0.5, 0.7, 1.0, 1.5))\n",
        "\n",
        "model.fit(lm_dataset, epochs=200, callbacks=[text_gen_callback])"
      ],
      "metadata": {
        "id": "NniKG2cUGD5E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}