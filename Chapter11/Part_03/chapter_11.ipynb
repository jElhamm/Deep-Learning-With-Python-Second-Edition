{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmRAEtvJ_6OT"
      },
      "source": [
        "# **Deep Learning With Python  -  CHAPTER 11**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HE2uMKK_7Qp"
      },
      "source": [
        "- This code provides a **modular approach** to **Sentiment Analysis** using **Bidirectional LSTMs** and **word embeddings (GloVe)**.\n",
        "\n",
        "- The `DatasetPreparer` manages data preparation, while `TextProcessor` standardizes and tokenizes text using **TextVectorization**. The `EmbeddingLoader` loads **pretrained word embeddings**, enabling improved performance.\n",
        "\n",
        "- The `SentimentModel` constructs **LSTM-based deep learning models**, and `Trainer` efficiently **trains and evaluates** the model. This structure ensures **scalability, readability, and reusability**, making it ideal for **NLP tasks**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "oqNg9frRCBgk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "7HHCQEtlu8Pg"
      },
      "outputs": [],
      "source": [
        "class DatasetDownloader:\n",
        "    @staticmethod\n",
        "    def download_and_extract():\n",
        "        os.system(\"curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
        "        os.system(\"tar -xf aclImdb_v1.tar.gz\")\n",
        "        os.system(\"rm -r aclImdb/train/unsup\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "lx1G3oUPCicE"
      },
      "outputs": [],
      "source": [
        "class DatasetPreparer:\n",
        "    def __init__(self, base_dir=\"aclImdb\"):\n",
        "        self.base_dir = pathlib.Path(base_dir)\n",
        "        self.train_dir = self.base_dir / \"train\"\n",
        "        self.val_dir = self.base_dir / \"val\"\n",
        "        self.batch_size = 32\n",
        "\n",
        "    def split_validation_data(self):\n",
        "        for category in (\"neg\", \"pos\"):\n",
        "            os.makedirs(self.val_dir / category, exist_ok=True)\n",
        "            files = os.listdir(self.train_dir / category)\n",
        "            random.Random(1337).shuffle(files)\n",
        "            num_val_samples = int(0.2 * len(files))\n",
        "            val_files = files[-num_val_samples:]\n",
        "            for fname in val_files:\n",
        "                shutil.move(self.train_dir / category / fname,\n",
        "                            self.val_dir / category / fname)\n",
        "\n",
        "    def load_datasets(self):\n",
        "        train_ds = keras.utils.text_dataset_from_directory(self.train_dir, batch_size=self.batch_size)\n",
        "        val_ds = keras.utils.text_dataset_from_directory(self.val_dir, batch_size=self.batch_size)\n",
        "        test_ds = keras.utils.text_dataset_from_directory(self.base_dir / \"test\", batch_size=self.batch_size)\n",
        "        return train_ds, val_ds, test_ds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self, max_tokens=20000, max_length=600):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.vectorizer = layers.TextVectorization(\n",
        "            max_tokens=max_tokens,\n",
        "            output_mode=\"int\",\n",
        "            output_sequence_length=max_length\n",
        "        )\n",
        "\n",
        "    def adapt(self, dataset):\n",
        "        text_only_dataset = dataset.map(lambda x, y: x)\n",
        "        self.vectorizer.adapt(text_only_dataset)\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        return dataset.map(lambda x, y: (self.vectorizer(x), y), num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "AMIFlb8B5DTO"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingLoader:\n",
        "    def __init__(self, glove_path=\"glove.6B.100d.txt\", max_tokens=20000, embedding_dim=100):\n",
        "        self.glove_path = glove_path\n",
        "        self.max_tokens = max_tokens\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings_index = self._load_glove_embeddings()\n",
        "\n",
        "    def _load_glove_embeddings(self):\n",
        "        embeddings_index = {}\n",
        "        with open(self.glove_path) as f:\n",
        "            for line in f:\n",
        "                word, coefs = line.split(maxsplit=1)\n",
        "                embeddings_index[word] = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        print(f\"Found {len(embeddings_index)} word vectors.\")\n",
        "        return embeddings_index\n",
        "\n",
        "    def create_embedding_matrix(self, vocabulary):\n",
        "        word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "        embedding_matrix = np.zeros((self.max_tokens, self.embedding_dim))\n",
        "        for word, i in word_index.items():\n",
        "            if i < self.max_tokens:\n",
        "                embedding_vector = self.embeddings_index.get(word)\n",
        "                if embedding_vector is not None:\n",
        "                    embedding_matrix[i] = embedding_vector\n",
        "        return embedding_matrix"
      ],
      "metadata": {
        "id": "zI1CcA9A5Fxm"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentModel:\n",
        "    @staticmethod\n",
        "    def build_lstm_model(max_tokens=20000, embedding_dim=256, use_pretrained_embedding=False, embedding_matrix=None):\n",
        "        inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "        if use_pretrained_embedding:\n",
        "            embedding_layer = layers.Embedding(\n",
        "                input_dim=max_tokens,\n",
        "                output_dim=embedding_dim,\n",
        "                embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                trainable=False,\n",
        "                mask_zero=True,\n",
        "            )\n",
        "        else:\n",
        "            embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=embedding_dim, mask_zero=True)\n",
        "\n",
        "        x = embedding_layer(inputs)\n",
        "        x = layers.Bidirectional(layers.LSTM(32))(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "        return model"
      ],
      "metadata": {
        "id": "FjL17ZVC5Iw-"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_ds, val_ds, model_name):\n",
        "        self.model = model\n",
        "        self.train_ds = train_ds\n",
        "        self.val_ds = val_ds\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def train(self, epochs=10):\n",
        "        callbacks = [keras.callbacks.ModelCheckpoint(f\"{self.model_name}.keras\", save_best_only=True)]\n",
        "        history = self.model.fit(self.train_ds.cache(),\n",
        "                                 validation_data=self.val_ds.cache(),\n",
        "                                 epochs=epochs,\n",
        "                                 callbacks=callbacks)\n",
        "        return history.history\n",
        "\n",
        "    def evaluate(self, test_ds):\n",
        "        best_model = keras.models.load_model(f\"{self.model_name}.keras\")\n",
        "        test_acc = best_model.evaluate(test_ds)[1]\n",
        "        print(f\"Test Accuracy: {test_acc:.3f}\")\n",
        "        return test_acc"
      ],
      "metadata": {
        "id": "XSAgrObR5KwV"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DatasetDownloader.download_and_extract()\n",
        "dataset_prep = DatasetPreparer()\n",
        "dataset_prep.split_validation_data()\n",
        "train_ds, val_ds, test_ds = dataset_prep.load_datasets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0JDNWw23iDd",
        "outputId": "fb75a336-4b20-4bfa-b65b-1a8013e83912"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = TextProcessor()\n",
        "processor.adapt(train_ds)\n",
        "train_ds = processor.transform(train_ds)\n",
        "val_ds = processor.transform(val_ds)\n",
        "test_ds = processor.transform(test_ds)"
      ],
      "metadata": {
        "id": "pjKcctas3nMV"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_loader = EmbeddingLoader()\n",
        "embedding_matrix = embedding_loader.create_embedding_matrix(processor.vectorizer.get_vocabulary())\n",
        "\n",
        "model = SentimentModel.build_lstm_model(use_pretrained_embedding=True, embedding_matrix=embedding_matrix)\n",
        "trainer = Trainer(model, train_ds, val_ds, \"glove_lstm_model\")\n",
        "trainer.train(epochs=10)\n",
        "trainer.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "zsHyqjgz5_h1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}