{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmRAEtvJ_6OT"
      },
      "source": [
        "# **Deep Learning With Python  -  CHAPTER 11**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HE2uMKK_7Qp"
      },
      "source": [
        "- This code provides a **structured and modular** implementation for **text preprocessing, vectorization, sentiment analysis modeling, and inference** using **TensorFlow/Keras**.\n",
        "\n",
        "- The `TextProcessor` handles **standardization and tokenization**, while the `TextVectorizer` converts raw text into numerical representations using **multi-hot encoding, TF-IDF, or N-grams**.\n",
        "\n",
        "- The `DatasetPreparer` organizes the **IMDB dataset**, and the `SentimentModel` constructs a **neural network** for binary classification. The `Trainer` manages **model training, evaluation, and checkpointing**, while the `Inference` class allows real-time **sentiment prediction** on new text inputs.\n",
        "\n",
        "- This modular approach ensures **scalability, efficiency, and ease of experimentation** in **NLP-based sentiment analysis**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "oqNg9frRCBgk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "7HHCQEtlu8Pg"
      },
      "outputs": [],
      "source": [
        "class TextProcessor:\n",
        "    @staticmethod\n",
        "    def standardize(text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        text = TextProcessor.standardize(text)\n",
        "        return text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "lx1G3oUPCicE"
      },
      "outputs": [],
      "source": [
        "class TextVectorizer:\n",
        "    def __init__(self, output_mode=\"multi_hot\", max_tokens=20000, ngrams=1):\n",
        "        self.vectorizer = keras.layers.TextVectorization(\n",
        "            max_tokens=max_tokens,\n",
        "            output_mode=output_mode,\n",
        "            ngrams=ngrams,\n",
        "            standardize=self.custom_standardization\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def custom_standardization(text):\n",
        "        text = tf.strings.lower(text)\n",
        "        return tf.strings.regex_replace(text, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "    def adapt(self, dataset):\n",
        "        text_only_dataset = dataset.map(lambda x, y: x)\n",
        "        self.vectorizer.adapt(text_only_dataset)\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        return dataset.map(lambda x, y: (self.vectorizer(x), y), num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetPreparer:\n",
        "    def __init__(self, base_dir=\"aclImdb\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.train_dir = os.path.join(base_dir, \"train\")\n",
        "        self.val_dir = os.path.join(base_dir, \"val\")\n",
        "        self.batch_size = 32\n",
        "\n",
        "    def split_validation_data(self):\n",
        "        for category in (\"neg\", \"pos\"):\n",
        "            os.makedirs(os.path.join(self.val_dir, category), exist_ok=True)\n",
        "            files = os.listdir(os.path.join(self.train_dir, category))\n",
        "            random.Random(1337).shuffle(files)\n",
        "            num_val_samples = int(0.2 * len(files))\n",
        "            val_files = files[-num_val_samples:]\n",
        "            for fname in val_files:\n",
        "                shutil.move(os.path.join(self.train_dir, category, fname),\n",
        "                            os.path.join(self.val_dir, category, fname))\n",
        "\n",
        "    def load_datasets(self):\n",
        "        train_ds = keras.utils.text_dataset_from_directory(self.train_dir, batch_size=self.batch_size)\n",
        "        val_ds = keras.utils.text_dataset_from_directory(self.val_dir, batch_size=self.batch_size)\n",
        "        test_ds = keras.utils.text_dataset_from_directory(os.path.join(self.base_dir, \"test\"), batch_size=self.batch_size)\n",
        "        return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "AMIFlb8B5DTO"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentModel:\n",
        "    @staticmethod\n",
        "    def build_model(max_tokens=20000, hidden_dim=16):\n",
        "        inputs = keras.Input(shape=(max_tokens,))\n",
        "        x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "        return model"
      ],
      "metadata": {
        "id": "zI1CcA9A5Fxm"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_ds, val_ds, model_name):\n",
        "        self.model = model\n",
        "        self.train_ds = train_ds\n",
        "        self.val_ds = val_ds\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def train(self, epochs=10):\n",
        "        callbacks = [keras.callbacks.ModelCheckpoint(f\"{self.model_name}.keras\", save_best_only=True)]\n",
        "        history = self.model.fit(self.train_ds.cache(),\n",
        "                                 validation_data=self.val_ds.cache(),\n",
        "                                 epochs=epochs,\n",
        "                                 callbacks=callbacks)\n",
        "        return history.history\n",
        "\n",
        "    def evaluate(self, test_ds):\n",
        "        best_model = keras.models.load_model(f\"{self.model_name}.keras\")\n",
        "        test_acc = best_model.evaluate(test_ds)[1]\n",
        "        print(f\"Test Accuracy: {test_acc:.3f}\")\n",
        "        return test_acc"
      ],
      "metadata": {
        "id": "FjL17ZVC5Iw-"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Inference:\n",
        "    def __init__(self, model, vectorizer):\n",
        "        self.model = model\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "    def predict_sentiment(self, text):\n",
        "        raw_text_data = tf.convert_to_tensor([[text]])\n",
        "        processed_inputs = self.vectorizer.vectorizer(raw_text_data)\n",
        "        prediction = self.model(processed_inputs)[0].numpy()\n",
        "        sentiment = \"positive\" if prediction > 0.5 else \"negative\"\n",
        "        confidence = float(prediction * 100)\n",
        "        print(f\"Sentiment: {sentiment} ({confidence:.2f}% confidence)\")\n",
        "        return sentiment, confidence"
      ],
      "metadata": {
        "id": "XSAgrObR5KwV"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_prep = DatasetPreparer()\n",
        "dataset_prep.split_validation_data()\n",
        "train_ds, val_ds, test_ds = dataset_prep.load_datasets()"
      ],
      "metadata": {
        "id": "_0JDNWw23iDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TextVectorizer(output_mode=\"multi_hot\", max_tokens=20000, ngrams=1)\n",
        "vectorizer.adapt(train_ds)\n",
        "train_ds = vectorizer.transform(train_ds)\n",
        "val_ds = vectorizer.transform(val_ds)\n",
        "test_ds = vectorizer.transform(test_ds)"
      ],
      "metadata": {
        "id": "pjKcctas3nMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentimentModel.build_model()\n",
        "trainer = Trainer(model, train_ds, val_ds, \"binary_1gram\")\n",
        "trainer.train(epochs=10)\n",
        "trainer.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "jFhwyijU3pBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "inference = Inference(inference_model, vectorizer)\n",
        "inference.predict_sentiment(\"That was an excellent movie, I loved it.\")"
      ],
      "metadata": {
        "id": "3bnGJpPw3tvw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}