{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmRAEtvJ_6OT"
      },
      "source": [
        "# **Deep Learning With Python  -  CHAPTER 11**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HE2uMKK_7Qp"
      },
      "source": [
        "- This code implements **sentiment analysis** on the **IMDB dataset** using a **Transformer-based neural network**. The data pipeline includes **text preprocessing**, **vectorization**, **custom Transformer Encoder**, and **Positional Embeddings**.\n",
        "\n",
        "- The model is **trained, validated, and evaluated** using `Trainer`. This **modular, reusable, and scalable** approach improves **readability** and allows **efficient experimentation** with different NLP architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oqNg9frRCBgk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7HHCQEtlu8Pg"
      },
      "outputs": [],
      "source": [
        "class DatasetDownloader:\n",
        "    @staticmethod\n",
        "    def download_and_extract():\n",
        "        os.system(\"curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
        "        os.system(\"tar -xf aclImdb_v1.tar.gz\")\n",
        "        os.system(\"rm -r aclImdb/train/unsup\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lx1G3oUPCicE"
      },
      "outputs": [],
      "source": [
        "class DatasetPreparer:\n",
        "    def __init__(self, base_dir=\"aclImdb\", batch_size=32):\n",
        "        self.base_dir = pathlib.Path(base_dir)\n",
        "        self.train_dir = self.base_dir / \"train\"\n",
        "        self.val_dir = self.base_dir / \"val\"\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def split_validation_data(self):\n",
        "        for category in (\"neg\", \"pos\"):\n",
        "            os.makedirs(self.val_dir / category, exist_ok=True)\n",
        "            files = os.listdir(self.train_dir / category)\n",
        "            random.Random(1337).shuffle(files)\n",
        "            num_val_samples = int(0.2 * len(files))\n",
        "            val_files = files[-num_val_samples:]\n",
        "            for fname in val_files:\n",
        "                shutil.move(self.train_dir / category / fname, self.val_dir / category / fname)\n",
        "\n",
        "    def load_datasets(self):\n",
        "        train_ds = keras.utils.text_dataset_from_directory(self.train_dir, batch_size=self.batch_size)\n",
        "        val_ds = keras.utils.text_dataset_from_directory(self.val_dir, batch_size=self.batch_size)\n",
        "        test_ds = keras.utils.text_dataset_from_directory(self.base_dir / \"test\", batch_size=self.batch_size)\n",
        "        return train_ds, val_ds, test_ds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self, max_tokens=20000, max_length=600):\n",
        "        self.vectorizer = layers.TextVectorization(\n",
        "            max_tokens=max_tokens, output_mode=\"int\", output_sequence_length=max_length)\n",
        "\n",
        "    def adapt(self, dataset):\n",
        "        text_only_dataset = dataset.map(lambda x, y: x)\n",
        "        self.vectorizer.adapt(text_only_dataset)\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        return dataset.map(lambda x, y: (self.vectorizer(x), y), num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "AMIFlb8B5DTO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "zI1CcA9A5Fxm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "FjL17ZVC5Iw-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerSentimentModel:\n",
        "    @staticmethod\n",
        "    def build(vocab_size=20000, sequence_length=600, embed_dim=256, num_heads=2, dense_dim=32):\n",
        "        inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "        x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "        x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "        x = layers.GlobalMaxPooling1D()(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "        return model"
      ],
      "metadata": {
        "id": "XSAgrObR5KwV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_ds, val_ds, model_name):\n",
        "        self.model = model\n",
        "        self.train_ds = train_ds\n",
        "        self.val_ds = val_ds\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def train(self, epochs=20):\n",
        "        callbacks = [keras.callbacks.ModelCheckpoint(f\"{self.model_name}.keras\", save_best_only=True)]\n",
        "        history = self.model.fit(self.train_ds.cache(),\n",
        "                                 validation_data=self.val_ds.cache(),\n",
        "                                 epochs=epochs,\n",
        "                                 callbacks=callbacks)\n",
        "        return history.history\n",
        "\n",
        "    def evaluate(self, test_ds):\n",
        "        best_model = keras.models.load_model(f\"{self.model_name}.keras\",\n",
        "                                             custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                                                             \"PositionalEmbedding\": PositionalEmbedding})\n",
        "        test_acc = best_model.evaluate(test_ds)[1]\n",
        "        print(f\"Test Accuracy: {test_acc:.3f}\")\n",
        "        return test_acc"
      ],
      "metadata": {
        "id": "iUSpYl9HBvpj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DatasetDownloader.download_and_extract()\n",
        "dataset_prep = DatasetPreparer()\n",
        "dataset_prep.split_validation_data()\n",
        "train_ds, val_ds, test_ds = dataset_prep.load_datasets()\n",
        "\n",
        "processor = TextProcessor()\n",
        "processor.adapt(train_ds)\n",
        "train_ds = processor.transform(train_ds)\n",
        "val_ds = processor.transform(val_ds)\n",
        "test_ds = processor.transform(test_ds)\n",
        "\n",
        "model = TransformerSentimentModel.build()\n",
        "trainer = Trainer(model, train_ds, val_ds, \"transformer_encoder\")\n",
        "trainer.train(epochs=20)\n",
        "trainer.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "_yXENvDlD1LT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}