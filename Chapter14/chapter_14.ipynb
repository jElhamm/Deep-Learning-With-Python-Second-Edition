{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4jVhBI-bTcHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmRAEtvJ_6OT"
      },
      "source": [
        "# **Deep Learning With Python  -  CHAPTER 14**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HE2uMKK_7Qp"
      },
      "source": [
        "- This script provides a modular and well-structured implementation of various deep learning models using TensorFlow and Keras.\n",
        "\n",
        "- It defines separate classes for different model architectures, including **Dense-based**, **CNN-based**, **LSTM-based**, and **Transformer-based** models. Each class encapsulates the logic for building and compiling models with configurable parameters, making the code highly reusable and flexible.\n",
        "\n",
        "- The `DenseModel` class handles fully connected networks for classification and regression tasks, `CNNModel` builds convolutional networks for image processing, `LSTMModel` constructs recurrent networks for sequence data, and `TransformerModel` implements transformer-based models for NLP tasks.\n",
        "\n",
        "- This structured approach improves readability, maintainability, and scalability, making it easier to extend or modify individual components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oqNg9frRCBgk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7HHCQEtlu8Pg"
      },
      "outputs": [],
      "source": [
        "class DenseModel:\n",
        "    \"\"\"Class for creating fully connected (Dense) models for different tasks.\"\"\"\n",
        "    def __init__(self, input_shape, num_units=32):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_units = num_units\n",
        "\n",
        "    def build_binary_classification_model(self):\n",
        "        inputs = keras.Input(shape=(self.input_shape,))\n",
        "        x = layers.Dense(self.num_units, activation=\"relu\")(inputs)\n",
        "        x = layers.Dense(self.num_units, activation=\"relu\")(x)\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")\n",
        "        return model\n",
        "\n",
        "    def build_multiclass_classification_model(self, num_classes):\n",
        "        inputs = keras.Input(shape=(self.input_shape,))\n",
        "        x = layers.Dense(self.num_units, activation=\"relu\")(inputs)\n",
        "        x = layers.Dense(self.num_units, activation=\"relu\")(x)\n",
        "        outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
        "        return model\n",
        "\n",
        "    def build_multilabel_classification_model(self, num_classes):\n",
        "        inputs = keras.Input(shape=(self.input_shape,))\n",
        "        x = layers.Dense(self.num_units, activation=\"relu\")(inputs)\n",
        "        x = layers.Dense(self.num_units, activation=\"relu\")(x)\n",
        "        outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")\n",
        "        return model\n",
        "\n",
        "    def build_regression_model(self, num_values):\n",
        "        inputs = keras.Input(shape=(self.input_shape,))\n",
        "        x = layers.Dense(self.num_units, activation=\"relu\")(inputs)\n",
        "        x = layers.Dense(self.num_units, activation=\"relu\")(x)\n",
        "        outputs = layers.Dense(num_values)(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel:\n",
        "    \"\"\"Class for creating CNN-based models.\"\"\"\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build_model(self):\n",
        "        inputs = keras.Input(shape=self.input_shape)\n",
        "        x = layers.SeparableConv2D(32, 3, activation=\"relu\")(inputs)\n",
        "        x = layers.SeparableConv2D(64, 3, activation=\"relu\")(x)\n",
        "        x = layers.MaxPooling2D(2)(x)\n",
        "        x = layers.SeparableConv2D(64, 3, activation=\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(128, 3, activation=\"relu\")(x)\n",
        "        x = layers.MaxPooling2D(2)(x)\n",
        "        x = layers.SeparableConv2D(64, 3, activation=\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(128, 3, activation=\"relu\")(x)\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "        x = layers.Dense(32, activation=\"relu\")(x)\n",
        "        outputs = layers.Dense(self.num_classes, activation=\"softmax\")(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
        "        return model"
      ],
      "metadata": {
        "id": "6mitfoIAJY0r"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel:\n",
        "    \"\"\"Class for creating LSTM-based models for sequence processing.\"\"\"\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build_single_layer_lstm(self):\n",
        "        inputs = keras.Input(shape=self.input_shape)\n",
        "        x = layers.LSTM(32)(inputs)\n",
        "        outputs = layers.Dense(self.num_classes, activation=\"sigmoid\")(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")\n",
        "        return model\n",
        "\n",
        "    def build_multi_layer_lstm(self):\n",
        "        inputs = keras.Input(shape=self.input_shape)\n",
        "        x = layers.LSTM(32, return_sequences=True)(inputs)\n",
        "        x = layers.LSTM(32, return_sequences=True)(x)\n",
        "        x = layers.LSTM(32)(x)\n",
        "        outputs = layers.Dense(self.num_classes, activation=\"sigmoid\")(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "qr8jz4GeLVbT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel:\n",
        "    \"\"\"Class for creating Transformer-based models.\"\"\"\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, dense_dim, num_heads):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def build_encoder_decoder_model(self):\n",
        "        encoder_inputs = keras.Input(shape=(self.sequence_length,), dtype=\"int64\")\n",
        "        x = PositionalEmbedding(self.sequence_length, self.vocab_size, self.embed_dim)(encoder_inputs)\n",
        "        encoder_outputs = TransformerEncoder(self.embed_dim, self.dense_dim, self.num_heads)(x)\n",
        "\n",
        "        decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "        x = PositionalEmbedding(self.sequence_length, self.vocab_size, self.embed_dim)(decoder_inputs)\n",
        "        x = TransformerDecoder(self.embed_dim, self.dense_dim, self.num_heads)(x, encoder_outputs)\n",
        "        decoder_outputs = layers.Dense(self.vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "        transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "        transformer.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
        "        return transformer\n",
        "\n",
        "    def build_text_classification_model(self):\n",
        "        inputs = keras.Input(shape=(self.sequence_length,), dtype=\"int64\")\n",
        "        x = PositionalEmbedding(self.sequence_length, self.vocab_size, self.embed_dim)(inputs)\n",
        "        x = TransformerEncoder(self.embed_dim, self.dense_dim, self.num_heads)(x)\n",
        "        x = layers.GlobalMaxPooling1D()(x)\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")\n",
        "        return model"
      ],
      "metadata": {
        "id": "5KU0n2MpLXsb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    num_input_features = 20\n",
        "    num_classes = 5\n",
        "    num_values = 1\n",
        "    height, width, channels = 32, 32, 3\n",
        "    num_timesteps, num_features = 10, 15\n",
        "    sequence_length, vocab_size, embed_dim, dense_dim, num_heads = 100, 20000, 64, 256, 8\n",
        "\n",
        "    dense_model = DenseModel(num_input_features)\n",
        "    binary_model = dense_model.build_binary_classification_model()\n",
        "    multi_class_model = dense_model.build_multiclass_classification_model(num_classes)\n",
        "    regression_model = dense_model.build_regression_model(num_values)\n",
        "\n",
        "    cnn_model = CNNModel((height, width, channels), num_classes).build_model()\n",
        "    lstm_model = LSTMModel((num_timesteps, num_features), num_classes)\n",
        "    single_lstm = lstm_model.build_single_layer_lstm()\n",
        "    multi_lstm = lstm_model.build_multi_layer_lstm()\n",
        "\n",
        "    transformer = TransformerModel(sequence_length, vocab_size, embed_dim, dense_dim, num_heads)\n",
        "    transformer_text_model = transformer.build_text_classification_model()"
      ],
      "metadata": {
        "id": "2Wr8ixy2TnVw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}